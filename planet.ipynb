{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a238fae-664b-47c8-95b5-88f5b3f733b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUJOCO_GL=egl\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%env MUJOCO_GL=egl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from dm_control import suite\n",
    "from dm_control.suite.wrappers import pixels\n",
    "from models import Encoder, Decoder, RewardModel, RSSM\n",
    "from mpc import MPC\n",
    "from replay import ExpReplay\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import display_img, display_video, preprocess_img\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de4c786-297b-4da3-973e-bfb08839b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For animations to render inline in jupyter,\n",
    "# download ffmpeg and set the path below to the location of the ffmpeg executable\n",
    "# plt.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664fc4fc-335a-4194-a04a-e143458f517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_EPS = 1\n",
    "TRAIN_EPS = 100\n",
    "UPDATES = 100\n",
    "ACTION_REPEAT = 8\n",
    "BATCH_SZ = 50\n",
    "CHUNK_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239a8883-e867-4cca-a203-a10e6691f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load('cartpole', 'swingup')\n",
    "env = pixels.Wrapper(env) # only use pixels instead of internal state\n",
    "act_spec = env.action_spec()\n",
    "action_dim = act_spec.shape[0]\n",
    "\n",
    "data = ExpReplay(BATCH_SZ, CHUNK_LEN, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef435f98-3e43-469d-8622-770963578352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward per ep:  26.938218960784745\n",
      "Avg timesteps per ep:  1000.0\n"
     ]
    }
   ],
   "source": [
    "# Generate random seed data\n",
    "total_reward_seed = 0\n",
    "t = 0\n",
    "for i in range(SEED_EPS):\n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    while not state.last():\n",
    "        t += 1\n",
    "        action = random_state.uniform(act_spec.minimum, act_spec.maximum, action_dim)\n",
    "        reward = state.reward\n",
    "        if reward is None: reward = 0\n",
    "        total_reward_seed += reward\n",
    "        frame = env.physics.render(camera_id=0, height=200, width=200)\n",
    "        frame = preprocess_img(frame).to(device)\n",
    "        data.append(frame, torch.as_tensor(action), torch.as_tensor(reward))\n",
    "        state = env.step(action)\n",
    "print(\"Avg reward per ep: \",total_reward_seed/SEED_EPS)\n",
    "print(\"Avg timesteps per ep: \", t/SEED_EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b12324-76b4-4818-9736-75fe2479b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "reward_model = RewardModel().to(device)\n",
    "rssm = RSSM(action_dim).to(device)\n",
    "params = list(enc.parameters()) + list(dec.parameters()) + list(reward_model.parameters()) + list(rssm.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3, eps=1e-4)\n",
    "\n",
    "planner = MPC(action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a705f9b-712d-4951-9fe0-2ba85fd9584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m stoc_state, _, _ \u001b[38;5;241m=\u001b[39m rssm\u001b[38;5;241m.\u001b[39mssm_posterior(det_state, enc(frame))\n\u001b[1;32m     22\u001b[0m stoc_state \u001b[38;5;241m=\u001b[39m stoc_state\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 23\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mplanner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstoc_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrssm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ACTION_REPEAT):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mlast(): \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/planet/mpc.py:30\u001b[0m, in \u001b[0;36mMPC.get_action\u001b[0;34m(self, det_state, stoc_state, rssm, reward_model)\u001b[0m\n\u001b[1;32m     28\u001b[0m         planning_det_state \u001b[38;5;241m=\u001b[39m rssm\u001b[38;5;241m.\u001b[39mdrnn(planning_det_state\u001b[38;5;241m.\u001b[39mto(device), planning_stoc_state\u001b[38;5;241m.\u001b[39mto(device), candidate_actions[c][t]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     29\u001b[0m         planning_stoc_state \u001b[38;5;241m=\u001b[39m rssm\u001b[38;5;241m.\u001b[39mssm_prior(planning_det_state\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 30\u001b[0m         planning_stoc_state \u001b[38;5;241m=\u001b[39m \u001b[43mplanning_stoc_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m()\n\u001b[1;32m     31\u001b[0m         horizon_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward_model(torch\u001b[38;5;241m.\u001b[39mcat((planning_det_state, planning_stoc_state))\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     32\u001b[0m candidate_rewards\u001b[38;5;241m.\u001b[39mappend(horizon_reward)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "# Train for 250 eps\n",
    "for i in range(1):\n",
    "    # MODEL FITTING\n",
    "\n",
    "    # DATA COLLECTION\n",
    "    t= 0\n",
    "    eps_reward = 0\n",
    "    with torch.no_grad():\n",
    "        state = env.reset()\n",
    "        det_state = torch.zeros(200).to(device)\n",
    "        stoc_state = torch.zeros(30).to(device)\n",
    "        action = torch.zeros(action_dim).to(device)\n",
    "        frame = preprocess_img(env.physics.render(camera_id=0, height=200, width=200)).to(device)\n",
    "        # while not state.last():\n",
    "        for i in range(10):\n",
    "            t+=1\n",
    "            print(t)\n",
    "            det_state = rssm.drnn(det_state, stoc_state, action.to(device))\n",
    "            stoc_state, _, _ = rssm.ssm_posterior(det_state, enc(frame))\n",
    "            stoc_state = stoc_state.squeeze()\n",
    "            action = planner.get_action(det_state.to(device), stoc_state.to(device), rssm, reward_model)\n",
    "            for _ in range(ACTION_REPEAT):\n",
    "                if state.last(): break\n",
    "                state = env.step(action)\n",
    "                eps_reward += state.reward\n",
    "            frame = preprocess_img(env.physics.render(camera_id=0, height=200, width=200))\n",
    "            data.append(frame, action, state.reward)\n",
    "        rewards.append(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd64df-a88a-425e-8dc5-e3d3e59adf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
