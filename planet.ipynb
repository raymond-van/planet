{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a238fae-664b-47c8-95b5-88f5b3f733b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%env MUJOCO_GL=egl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "from dm_control import suite\n",
    "from dm_control.suite.wrappers import pixels\n",
    "from models import Encoder, Decoder, RewardModel, RSSM\n",
    "from mpc import MPC\n",
    "from replay import ExpReplay\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import display_img, display_video, preprocess_img, save_data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4c786-297b-4da3-973e-bfb08839b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For animations to render inline in jupyter,\n",
    "# download ffmpeg and set the path below to the location of the ffmpeg executable\n",
    "# plt.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664fc4fc-335a-4194-a04a-e143458f517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_EPS = 5\n",
    "TRAIN_EPS = 100\n",
    "UPDATES = 100\n",
    "BATCH_SZ = 50\n",
    "CHUNK_LEN = 50\n",
    "LOAD_WEIGHTS = 149\n",
    "DOMAIN_TASK = \"cheetah_run\"\n",
    "domain, task = DOMAIN_TASK.split(\"_\")\n",
    "ACTION_REPEAT = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a8883-e867-4cca-a203-a10e6691f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load(domain, task, task_kwargs={'random': random_state})\n",
    "env = pixels.Wrapper(env) # only use pixels instead of internal state\n",
    "act_spec = env.action_spec()\n",
    "action_dim = act_spec.shape[0]\n",
    "\n",
    "data = ExpReplay(BATCH_SZ, CHUNK_LEN, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef435f98-3e43-469d-8622-770963578352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_action_gif = []\n",
    "# Generate random seed data\n",
    "rand_rewards = []\n",
    "total_reward_seed = 0\n",
    "for i in range(SEED_EPS):\n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    ep_reward = 0\n",
    "    while not state.last():\n",
    "        action = random_state.uniform(act_spec.minimum, act_spec.maximum, action_dim)\n",
    "        reward = state.reward\n",
    "        if reward is None: reward = 0\n",
    "        ep_reward += reward\n",
    "        frame = env.physics.render(camera_id=0, height=64, width=64)\n",
    "        # rand_action_gif.append(frame)\n",
    "        frame = preprocess_img(frame).to(device)\n",
    "        data.append(frame, torch.as_tensor(action), torch.as_tensor(reward))\n",
    "        state = env.step(action)\n",
    "    rand_rewards.append(ep_reward)\n",
    "print(\"Random Action - Avg reward/ep: \",total_reward_seed/SEED_EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b12324-76b4-4818-9736-75fe2479b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "reward_model = RewardModel().to(device)\n",
    "rssm = RSSM(action_dim).to(device)\n",
    "params = list(enc.parameters()) + list(dec.parameters()) + list(reward_model.parameters()) + list(rssm.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3, eps=1e-4)\n",
    "\n",
    "planner = MPC(action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a3d71-dc94-4170-8ba3-dbe02a8fd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_list = []\n",
    "losses_list = []\n",
    "observations = []\n",
    "free_nats = torch.tensor(3).to(device)\n",
    "TRAIN_EPS = 150\n",
    "# Train for 150 eps\n",
    "for ep in tqdm(range(LOAD_WEIGHTS+1, TRAIN_EPS)):\n",
    "    # MODEL FITTING\n",
    "    total_loss = 0\n",
    "    for j in range(UPDATES):\n",
    "        obs, actions, rewards = data.sample_batch()\n",
    "        state = env.reset()\n",
    "        obs_loss, reward_loss, kl_div = 0, 0, 0\n",
    "        det_state = torch.zeros((50,200)).to(device)\n",
    "        stoc_state = torch.zeros((50,30)).to(device)\n",
    "        prior_mean, prior_dev, post_mean, post_dev = torch.zeros((50,30)).to(device), torch.zeros((50,30)).to(device), \\\n",
    "                                                     torch.zeros((50,30)).to(device), torch.zeros((50,30)).to(device)\n",
    "        for b in range(BATCH_SZ):\n",
    "            det_state = rssm.drnn(det_state, stoc_state, actions[b])\n",
    "            prior_state, prior_mean, prior_dev = rssm.ssm_prior(det_state)\n",
    "            posterior_state, post_dev, post_dev = rssm.ssm_posterior(det_state, enc(obs[b]))\n",
    "            obs_loss += F.mse_loss(dec(det_state, stoc_state), obs[b])\n",
    "            reward_loss += F.mse_loss(reward_model(torch.cat((det_state, stoc_state),dim=1)), rewards[b])\n",
    "            prior_state_dist = torch.distributions.Normal(prior_mean, prior_dev)\n",
    "            posterior_state_dist = torch.distributions.Normal(post_mean, post_dev)\n",
    "            kl_div += torch.max(torch.distributions.kl_divergence(prior_state_dist, posterior_state_dist).sum(1), free_nats).mean()\n",
    "        optimizer.zero_grad()\n",
    "        torch.nn.utils.clip_grad_norm_(params, 1000., norm_type=2)\n",
    "        loss = obs_loss + reward_loss + kl_div\n",
    "        loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses_list.append(total_loss)\n",
    "    # DATA COLLECTION\n",
    "    eps_reward = 0\n",
    "    vid = []\n",
    "    state = env.reset()\n",
    "    frame = env.physics.render(camera_id=0, height=64, width=64)\n",
    "    with torch.no_grad():\n",
    "        det_state = torch.zeros(200).to(device)\n",
    "        stoc_state = torch.zeros(30).to(device)\n",
    "        action = torch.zeros(action_dim).to(device)\n",
    "        frame = preprocess_img(env.physics.render(camera_id=0, height=200, width=200)).to(device)\n",
    "        while not state.last():\n",
    "            det_state = rssm.drnn(det_state, stoc_state, action.to(device))\n",
    "            stoc_state, _, _ = rssm.ssm_posterior(det_state, enc(frame))\n",
    "            stoc_state = stoc_state.squeeze()\n",
    "            action = planner.get_action(det_state.to(device), stoc_state.to(device), rssm, reward_model)\n",
    "            for _ in range(ACTION_REPEAT):\n",
    "                if state.last(): break\n",
    "                state = env.step(action)\n",
    "                eps_reward += state.reward\n",
    "                frame = env.physics.render(camera_id=0, height=64, width=64)\n",
    "                vid.append(frame)\n",
    "            frame = preprocess_img(frame).to(device)\n",
    "            data.append(frame, action, state.reward)\n",
    "        rewards_list.append(eps_reward)\n",
    "        observations.append(vid)\n",
    "    if (ep % 10 == 0):\n",
    "        torch.save({'rssm': rssm.state_dict(), 'decoder': dec.state_dict(), \\\n",
    "                    'reward': reward_model.state_dict(), 'encoder': enc.state_dict()}, \\\n",
    "                     os.path.join('weights', '{task}_{eps}.pth'.format(task=DOMAIN_TASK, eps=ep)))\n",
    "        anim = display_video(vid, return_anim=True)\n",
    "        anim.save('gifs/{task}_{eps}.gif'.format(task=DOMAIN_TASK, eps=ep), writer='imagemagick', fps=30)\n",
    "    print(\"Episode: {}/{}, Loss = {:.2f}, Reward = {:.2f}\".format(ep, TRAIN_EPS, total_loss, eps_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105f660-6688-4147-b223-7d3a225eb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f3668-7481-4b51-a9dc-53c2859bba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(losses_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
